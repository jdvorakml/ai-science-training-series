{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write an elementary tokenizer that uses words as tokens.\n",
    "\n",
    "We will use Mark Twain's _Life On The Mississippi_ as a test bed. The text is in the accompanying file 'Life_On_The_Mississippi.txt'\n",
    "\n",
    "Here's a not-terribly-good such tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n",
      "('the', 10012)\n",
      "('project', 90)\n",
      "('gutenberg', 87)\n",
      "('ebook', 13)\n",
      "('of', 4532)\n",
      "('life', 89)\n",
      "('on', 947)\n",
      "('mississippi', 159)\n",
      "('this', 781)\n",
      "('is', 1148)\n",
      "('for', 1095)\n",
      "('use', 48)\n",
      "('anone', 5)\n",
      "('anwhere', 18)\n",
      "('in', 2593)\n",
      "('united', 37)\n",
      "('states', 54)\n",
      "('and', 5892)\n",
      "('most', 124)\n",
      "('other', 270)\n",
      "('parts', 10)\n",
      "('world', 68)\n",
      "('at', 750)\n",
      "('no', 422)\n",
      "('cost', 25)\n",
      "('with', 1081)\n",
      "('almost', 38)\n",
      "('restrictions', 2)\n",
      "('whatsoever', 2)\n",
      "('you', 117)\n",
      "('ma', 90)\n",
      "('cop', 17)\n",
      "('it', 2294)\n",
      "('give', 81)\n",
      "('awa', 172)\n",
      "('or', 581)\n",
      "('reuse', 2)\n",
      "('under', 119)\n",
      "('terms', 26)\n",
      "('license', 24)\n",
      "('included', 3)\n",
      "('online', 4)\n",
      "('wwwgutenbergorg', 5)\n",
      "('if', 381)\n",
      "('ou', 916)\n",
      "('are', 387)\n",
      "('not', 722)\n",
      "('located', 9)\n",
      "('will', 301)\n",
      "('have', 571)\n",
      "('to', 3593)\n",
      "('check', 4)\n",
      "('laws', 17)\n",
      "('countr', 76)\n",
      "('where', 174)\n",
      "('before', 208)\n",
      "('using', 11)\n",
      "('title', 3)\n",
      "('author', 3)\n",
      "('mark', 24)\n",
      "('twain', 26)\n",
      "('release', 1)\n",
      "('date', 18)\n",
      "('jul', 7)\n",
      "('10', 10)\n",
      "('2004', 1)\n",
      "('245', 1)\n",
      "('recentl', 4)\n",
      "('updated', 2)\n",
      "('januar', 3)\n",
      "('1', 13)\n",
      "('2021', 1)\n",
      "('language', 12)\n",
      "('english', 11)\n",
      "('credits', 1)\n",
      "('produced', 22)\n",
      "('b', 726)\n",
      "('david', 2)\n",
      "('widger', 2)\n",
      "('earliest', 7)\n",
      "('pg', 3)\n",
      "('text', 4)\n",
      "('edition', 4)\n",
      "('graham', 2)\n",
      "('allan', 2)\n",
      "('start', 31)\n",
      "('table', 6)\n",
      "('contents', 6)\n",
      "('chapter', 125)\n",
      "('i', 2205)\n",
      "('well', 191)\n",
      "('worth', 43)\n",
      "('reading', 13)\n",
      "('aboutit', 1)\n",
      "('remarkableinstead', 1)\n",
      "('widening', 2)\n",
      "('towards', 9)\n",
      "('its', 323)\n",
      "('mouth', 53)\n",
      "('grows', 3)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# HOMEWORK Part 1\n",
    "punctuations = string.punctuation\n",
    "trans_table = str.maketrans('', '', punctuations)\n",
    "print(trans_table)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "wdict = {}\n",
    "with open('Life_On_The_Mississippi.txt', 'r') as L:\n",
    "    line = L.readline()\n",
    "    nlines = 1\n",
    "    while line:\n",
    "        # ----------------------------------------------------------------------\n",
    "        # HOMEWORK Part 1 \n",
    "        # remove punctuation chaterecters for line\n",
    "        line = line.translate(trans_table)\n",
    "        # remove non-ASCII characters\n",
    "        line = re.sub(r'[^\\x00-x7f]', '', line)\n",
    "        # to lowercase\n",
    "        line = line.lower()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if wdict.get(word) is not None:\n",
    "                wdict[word] += 1\n",
    "            else:\n",
    "                wdict[word] = 1\n",
    "        line = L.readline()\n",
    "        nlines += 1\n",
    "\n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break\n",
    "\n",
    "#print(len(wdict))\n",
    "#print(wdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unsatisfactory for a few reasons:\n",
    "\n",
    "* There are non-ASCII (Unicode) characters that should be stripped (the so-called \"Byte-Order Mark\" or BOM \\ufeff at the beginning of the text);\n",
    "\n",
    "* There are punctuation marks, which we don't want to concern ourselves with;\n",
    "\n",
    "* The same word can appear capitalized, or lower-case, or with its initial letter upper-cased, whereas we want them all to be normalized to lower-case.\n",
    "\n",
    "Part 1 of this assignment: insert code in this loop to operate on the str variable 'line' so as to fix these problems before 'line' is split into words.\n",
    "\n",
    "A hint to one possible way to do this: use the 'punctuation' character definition in the Python 'string' module, the 'maketrans' and 'translate' methods of Python's str class, to eliminate punctuation, and the regular expression ('re') Python module to eliminate any Unicode---it is useful to know that the regular expression r'[^\\x00-x7f]' means \"any character not in the vanilla ASCII set.\n",
    "\n",
    "Part 2: Add code to sort the contents of wdict by word occurrence frequency.  What are the top 100 most frequent word tokens?  Adding up occurrence frequencies starting from the most frequent words, how many distinct words make up the top 90% of word occurrences in this \"corpus\"?\n",
    "\n",
    "For this part, the docs of Python's 'sorted' and of the helper 'itemgetter' from 'operator' reward study.\n",
    "\n",
    "Write your modified code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The most frequent words:\n",
      "--------------------------------------------------------------------------------\n",
      "    1: word = the                  occurence frequency = 10012\n",
      "    2: word = and                  occurence frequency = 5892\n",
      "    3: word = of                   occurence frequency = 4532\n",
      "    4: word = a                    occurence frequency = 4055\n",
      "    5: word = to                   occurence frequency = 3593\n",
      "    6: word = in                   occurence frequency = 2593\n",
      "    7: word = it                   occurence frequency = 2294\n",
      "    8: word = i                    occurence frequency = 2205\n",
      "    9: word = was                  occurence frequency = 2114\n",
      "   10: word = that                 occurence frequency = 1724\n",
      "   11: word = he                   occurence frequency = 1402\n",
      "   12: word = is                   occurence frequency = 1148\n",
      "   13: word = for                  occurence frequency = 1095\n",
      "   14: word = with                 occurence frequency = 1081\n",
      "   15: word = his                  occurence frequency = 961\n",
      "   16: word = had                  occurence frequency = 961\n",
      "   17: word = but                  occurence frequency = 952\n",
      "   18: word = on                   occurence frequency = 947\n",
      "   19: word = ou                   occurence frequency = 916\n",
      "   20: word = as                   occurence frequency = 881\n",
      "   21: word = this                 occurence frequency = 781\n",
      "   22: word = at                   occurence frequency = 750\n",
      "   23: word = b                    occurence frequency = 726\n",
      "   24: word = not                  occurence frequency = 722\n",
      "   25: word = all                  occurence frequency = 720\n",
      "   26: word = one                  occurence frequency = 688\n",
      "   27: word = an                   occurence frequency = 675\n",
      "   28: word = there                occurence frequency = 629\n",
      "   29: word = were                 occurence frequency = 625\n",
      "   30: word = be                   occurence frequency = 617\n",
      "   31: word = m                    occurence frequency = 583\n",
      "   32: word = or                   occurence frequency = 581\n",
      "   33: word = from                 occurence frequency = 577\n",
      "   34: word = have                 occurence frequency = 571\n",
      "   35: word = out                  occurence frequency = 541\n",
      "   36: word = so                   occurence frequency = 537\n",
      "   37: word = up                   occurence frequency = 529\n",
      "   38: word = him                  occurence frequency = 523\n",
      "   39: word = we                   occurence frequency = 519\n",
      "   40: word = me                   occurence frequency = 516\n",
      "   41: word = when                 occurence frequency = 505\n",
      "   42: word = would                occurence frequency = 478\n",
      "   43: word = which                occurence frequency = 476\n",
      "   44: word = river                occurence frequency = 457\n",
      "   45: word = them                 occurence frequency = 425\n",
      "   46: word = no                   occurence frequency = 422\n",
      "   47: word = then                 occurence frequency = 405\n",
      "   48: word = said                 occurence frequency = 399\n",
      "   49: word = are                  occurence frequency = 387\n",
      "   50: word = man                  occurence frequency = 383\n",
      "   51: word = if                   occurence frequency = 381\n",
      "   52: word = their                occurence frequency = 378\n",
      "   53: word = now                  occurence frequency = 369\n",
      "   54: word = about                occurence frequency = 346\n",
      "   55: word = time                 occurence frequency = 337\n",
      "   56: word = been                 occurence frequency = 335\n",
      "   57: word = down                 occurence frequency = 328\n",
      "   58: word = its                  occurence frequency = 323\n",
      "   59: word = ever                 occurence frequency = 323\n",
      "   60: word = could                occurence frequency = 313\n",
      "   61: word = has                  occurence frequency = 305\n",
      "   62: word = will                 occurence frequency = 301\n",
      "   63: word = into                 occurence frequency = 300\n",
      "   64: word = what                 occurence frequency = 285\n",
      "   65: word = our                  occurence frequency = 280\n",
      "   66: word = her                  occurence frequency = 278\n",
      "   67: word = two                  occurence frequency = 273\n",
      "   68: word = do                   occurence frequency = 271\n",
      "   69: word = other                occurence frequency = 270\n",
      "   70: word = some                 occurence frequency = 269\n",
      "   71: word = new                  occurence frequency = 259\n",
      "   72: word = got                  occurence frequency = 234\n",
      "   73: word = these                occurence frequency = 233\n",
      "   74: word = she                  occurence frequency = 233\n",
      "   75: word = who                  occurence frequency = 229\n",
      "   76: word = more                 occurence frequency = 226\n",
      "   77: word = water                occurence frequency = 225\n",
      "   78: word = did                  occurence frequency = 214\n",
      "   79: word = before               occurence frequency = 208\n",
      "   80: word = over                 occurence frequency = 202\n",
      "   81: word = wa                   occurence frequency = 202\n",
      "   82: word = hundred              occurence frequency = 200\n",
      "   83: word = upon                 occurence frequency = 200\n",
      "   84: word = here                 occurence frequency = 199\n",
      "   85: word = after                occurence frequency = 195\n",
      "   86: word = da                   occurence frequency = 193\n",
      "   87: word = than                 occurence frequency = 192\n",
      "   88: word = well                 occurence frequency = 191\n",
      "   89: word = through              occurence frequency = 191\n",
      "   90: word = get                  occurence frequency = 190\n",
      "   91: word = old                  occurence frequency = 186\n",
      "   92: word = can                  occurence frequency = 185\n",
      "   93: word = boat                 occurence frequency = 184\n",
      "   94: word = went                 occurence frequency = 183\n",
      "   95: word = never                occurence frequency = 182\n",
      "   96: word = good                 occurence frequency = 181\n",
      "   97: word = ears                 occurence frequency = 180\n",
      "   98: word = see                  occurence frequency = 176\n",
      "   99: word = know                 occurence frequency = 175\n",
      "  100: word = where                occurence frequency = 174\n",
      "################################################################################\n",
      "total number of distinct words = 14352\n",
      "total number of occurences of all words = 147419\n",
      "the first 3676 most frequent words makes up the top 90.0 % of word occurences\n"
     ]
    }
   ],
   "source": [
    "# HOMEWORK Part 1\n",
    "import numpy as np\n",
    "\n",
    "def sort_words_by_frequency(words):\n",
    "\n",
    "    # sort based on frequency in descending order\n",
    "    sorted_words = sorted(words.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    return sorted_words\n",
    "\n",
    "\n",
    "def print_first_n_words_by_occurence_frequency(sorted_words, n=100):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('The most frequent words:')\n",
    "    print('-' * 80)\n",
    "    for i in range(n):\n",
    "        print(f'{i+1:5}: word = {sorted_words[i][0]:20} occurence frequency = {sorted_words[i][1]}')\n",
    "    print('#' * 80)\n",
    "\n",
    "\n",
    "def get_number_of_words_fraction_occurence(sorted_words, fraction=0.9):\n",
    "\n",
    "    # convert list of tuples to 2D numpy array\n",
    "    data = np.array(sorted_words, dtype=object)\n",
    "    occurences = data[:,1]\n",
    "    total = np.sum(occurences)\n",
    "    print(f'total number of distinct words = {len(occurences)}')\n",
    "    print(f'total number of occurences of all words = {total}')\n",
    "    partial = 0\n",
    "    nwords = 0\n",
    "    for occurence in occurences:\n",
    "        #print(partial, partial / total)\n",
    "        if partial / total > fraction:\n",
    "            break\n",
    "        partial += occurence\n",
    "        nwords += 1\n",
    "\n",
    "    return nwords\n",
    "\n",
    "\n",
    "sorted_words = sort_words_by_frequency(wdict)\n",
    "print_first_n_words_by_occurence_frequency(sorted_words, n=100)\n",
    "\n",
    "fraction = 0.9\n",
    "nwords = get_number_of_words_fraction_occurence(sorted_words, fraction=fraction)\n",
    "print(f'the first {nwords} most frequent words makes up the top {fraction*100:.1f} % of word occurences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HOMEWORK Part 2, see the end of \"Sequential_Data_Models.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
